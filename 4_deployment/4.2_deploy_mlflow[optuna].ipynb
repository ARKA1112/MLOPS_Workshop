{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to train a model using mlflow as a tracking server and then log its artifact.\n",
    "\n",
    "// Then we need to create a separate directory and bring in all the Pipfile, Pipfile.lock , predict.py, and test.py file\n",
    "\n",
    "// We need to modify the predict.py file in order to load the model trained in mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Next we need to configure the aws credentials setting ```AWS_SECRET_ID``` and ```AWS_SECRET_ACCESS_KEY``` as environment variables by typing the following commands in the bash\n",
    "\n",
    "```javascript                   \n",
    "export AWS_SECRET_ID = Ay78***********\n",
    "```\n",
    "and\n",
    "```javascript\n",
    "export AWS_SECRET_ACCESS_KEY = *************\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//Afterwards we need to start the mlflow server using the following commands\n",
    "\n",
    "```cli\n",
    "mflow server -h 0.0.0.0 --backend-store-uri sqlite:///mlflow.db --default-artifact-root= S3://<bucket name>\n",
    "```\n",
    "\n",
    "Then we should create an experiment for which the artifact store must be the same\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "mlflow.create_experiment(\"exp_name\",artifact_location=\"S3://<bucket_name>/path\")\n",
    "```\n",
    "\n",
    "However while logging the model, if we execute the following code\n",
    "```python\n",
    "mlflow.sklearn.log_model(model_name, artifact_path=\"model\")\n",
    "```\n",
    "\n",
    "The model will be stored in the location: ```S3://<bucket_name>/path/model```\n",
    "\n",
    "\n",
    "Doing these steps enables us to retrieve the artifacts when required, without running the server locally, one can easily access the artifacts from s3 buckets provided they have the necessary credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-01 00:03:03</td>\n",
       "      <td>2023-11-01 01:04:08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>61.8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-01 00:03:28</td>\n",
       "      <td>2023-11-01 00:23:59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.60</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-31 23:58:05</td>\n",
       "      <td>2023-11-01 00:54:03</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.61</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16.54</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.23</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-01 00:03:50</td>\n",
       "      <td>2023-11-01 00:04:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.28</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-11-01 00:06:30</td>\n",
       "      <td>2023-11-01 00:14:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2023-11-01 00:03:03   2023-11-01 01:04:08              2.0   \n",
       "1         1  2023-11-01 00:03:28   2023-11-01 00:23:59              0.0   \n",
       "2         2  2023-10-31 23:58:05   2023-11-01 00:54:03              4.0   \n",
       "3         2  2023-11-01 00:03:50   2023-11-01 00:04:59              1.0   \n",
       "4         2  2023-11-01 00:06:30   2023-11-01 00:14:25              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0          13.60         1.0                  N           132            26   \n",
       "1           3.50         1.0                  N           140             7   \n",
       "2          18.61         2.0                  N           132           230   \n",
       "3           0.39         1.0                  N           236           236   \n",
       "4           1.20         1.0                  N           236           141   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2         61.8   2.75      0.5        0.00          0.00   \n",
       "1             1         20.5   3.50      0.5        5.10          0.00   \n",
       "2             1         70.0   0.00      0.5       16.54          6.94   \n",
       "3             1          4.4   1.00      0.5        1.88          0.00   \n",
       "4             1         10.0   1.00      0.5        3.00          0.00   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
       "0                    1.0         66.05                   0.0         1.75  \n",
       "1                    1.0         30.60                   2.5         0.00  \n",
       "2                    1.0         99.23                   2.5         1.75  \n",
       "3                    1.0         11.28                   2.5         0.00  \n",
       "4                    1.0         18.00                   2.5         0.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'Airport_fee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2671772.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data['duration'] = data['tpep_dropoff_datetime'] - data['tpep_pickup_datetime']\n",
    "    data['duration'] = data['duration'].apply(lambda td: td.total_seconds()/60)\n",
    "    X = data[['passenger_count','trip_distance','PULocationID','DOLocationID','fare_amount']]\n",
    "    y = data['duration']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3339715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18366/1507299739.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['duration'] = data['tpep_dropoff_datetime'] - data['tpep_pickup_datetime']\n",
      "/tmp/ipykernel_18366/1507299739.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['duration'] = data['duration'].apply(lambda td: td.total_seconds()/60)\n",
      "/tmp/ipykernel_18366/1507299739.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['duration'] = data['tpep_dropoff_datetime'] - data['tpep_pickup_datetime']\n",
      "/tmp/ipykernel_18366/1507299739.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['duration'] = data['duration'].apply(lambda td: td.total_seconds()/60)\n"
     ]
    }
   ],
   "source": [
    "X_train_val, y_train_val = data_preparation(data[:20000])\n",
    "X_test, y_test = data_preparation(data[20001:25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_train_val,y_train_val, test_size=0.3,random_state=44,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr = X_train.to_dict(orient='records')\n",
    "xval = X_val.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dv.fit_transform(xtr)\n",
    "val_data = dv.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and optuna hyperparameter tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'squared_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForestRegressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    A random forest regressor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    A random forest is a meta estimator that fits a number of classifying\u001b[0m\n",
      "\u001b[0;34m    decision trees on various sub-samples of the dataset and uses averaging\u001b[0m\n",
      "\u001b[0;34m    to improve the predictive accuracy and control over-fitting.\u001b[0m\n",
      "\u001b[0;34m    The sub-sample size is controlled with the `max_samples` parameter if\u001b[0m\n",
      "\u001b[0;34m    `bootstrap=True` (default), otherwise the whole dataset is used to build\u001b[0m\n",
      "\u001b[0;34m    each tree.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For a comparison between tree-based ensemble models see the example\u001b[0m\n",
      "\u001b[0;34m    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Read more in the :ref:`User Guide <forest>`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Parameters\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    n_estimators : int, default=100\u001b[0m\n",
      "\u001b[0;34m        The number of trees in the forest.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.22\u001b[0m\n",
      "\u001b[0;34m           The default value of ``n_estimators`` changed from 10 to 100\u001b[0m\n",
      "\u001b[0;34m           in 0.22.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, \\\u001b[0m\n",
      "\u001b[0;34m            default=\"squared_error\"\u001b[0m\n",
      "\u001b[0;34m        The function to measure the quality of a split. Supported criteria\u001b[0m\n",
      "\u001b[0;34m        are \"squared_error\" for the mean squared error, which is equal to\u001b[0m\n",
      "\u001b[0;34m        variance reduction as feature selection criterion and minimizes the L2\u001b[0m\n",
      "\u001b[0;34m        loss using the mean of each terminal node, \"friedman_mse\", which uses\u001b[0m\n",
      "\u001b[0;34m        mean squared error with Friedman's improvement score for potential\u001b[0m\n",
      "\u001b[0;34m        splits, \"absolute_error\" for the mean absolute error, which minimizes\u001b[0m\n",
      "\u001b[0;34m        the L1 loss using the median of each terminal node, and \"poisson\" which\u001b[0m\n",
      "\u001b[0;34m        uses reduction in Poisson deviance to find splits.\u001b[0m\n",
      "\u001b[0;34m        Training using \"absolute_error\" is significantly slower\u001b[0m\n",
      "\u001b[0;34m        than when using \"squared_error\".\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.18\u001b[0m\n",
      "\u001b[0;34m           Mean Absolute Error (MAE) criterion.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.0\u001b[0m\n",
      "\u001b[0;34m           Poisson criterion.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_depth : int, default=None\u001b[0m\n",
      "\u001b[0;34m        The maximum depth of the tree. If None, then nodes are expanded until\u001b[0m\n",
      "\u001b[0;34m        all leaves are pure or until all leaves contain less than\u001b[0m\n",
      "\u001b[0;34m        min_samples_split samples.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_samples_split : int or float, default=2\u001b[0m\n",
      "\u001b[0;34m        The minimum number of samples required to split an internal node:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If int, then consider `min_samples_split` as the minimum number.\u001b[0m\n",
      "\u001b[0;34m        - If float, then `min_samples_split` is a fraction and\u001b[0m\n",
      "\u001b[0;34m          `ceil(min_samples_split * n_samples)` are the minimum\u001b[0m\n",
      "\u001b[0;34m          number of samples for each split.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
      "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_samples_leaf : int or float, default=1\u001b[0m\n",
      "\u001b[0;34m        The minimum number of samples required to be at a leaf node.\u001b[0m\n",
      "\u001b[0;34m        A split point at any depth will only be considered if it leaves at\u001b[0m\n",
      "\u001b[0;34m        least ``min_samples_leaf`` training samples in each of the left and\u001b[0m\n",
      "\u001b[0;34m        right branches.  This may have the effect of smoothing the model,\u001b[0m\n",
      "\u001b[0;34m        especially in regression.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If int, then consider `min_samples_leaf` as the minimum number.\u001b[0m\n",
      "\u001b[0;34m        - If float, then `min_samples_leaf` is a fraction and\u001b[0m\n",
      "\u001b[0;34m          `ceil(min_samples_leaf * n_samples)` are the minimum\u001b[0m\n",
      "\u001b[0;34m          number of samples for each node.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
      "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_weight_fraction_leaf : float, default=0.0\u001b[0m\n",
      "\u001b[0;34m        The minimum weighted fraction of the sum total of weights (of all\u001b[0m\n",
      "\u001b[0;34m        the input samples) required to be at a leaf node. Samples have\u001b[0m\n",
      "\u001b[0;34m        equal weight when sample_weight is not provided.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\u001b[0m\n",
      "\u001b[0;34m        The number of features to consider when looking for the best split:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If int, then consider `max_features` features at each split.\u001b[0m\n",
      "\u001b[0;34m        - If float, then `max_features` is a fraction and\u001b[0m\n",
      "\u001b[0;34m          `max(1, int(max_features * n_features_in_))` features are considered at each\u001b[0m\n",
      "\u001b[0;34m          split.\u001b[0m\n",
      "\u001b[0;34m        - If \"sqrt\", then `max_features=sqrt(n_features)`.\u001b[0m\n",
      "\u001b[0;34m        - If \"log2\", then `max_features=log2(n_features)`.\u001b[0m\n",
      "\u001b[0;34m        - If None or 1.0, then `max_features=n_features`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. note::\u001b[0m\n",
      "\u001b[0;34m            The default of 1.0 is equivalent to bagged trees and more\u001b[0m\n",
      "\u001b[0;34m            randomness can be achieved by setting smaller values, e.g. 0.3.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 1.1\u001b[0m\n",
      "\u001b[0;34m            The default of `max_features` changed from `\"auto\"` to 1.0.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Note: the search for a split does not stop until at least one\u001b[0m\n",
      "\u001b[0;34m        valid partition of the node samples is found, even if it requires to\u001b[0m\n",
      "\u001b[0;34m        effectively inspect more than ``max_features`` features.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_leaf_nodes : int, default=None\u001b[0m\n",
      "\u001b[0;34m        Grow trees with ``max_leaf_nodes`` in best-first fashion.\u001b[0m\n",
      "\u001b[0;34m        Best nodes are defined as relative reduction in impurity.\u001b[0m\n",
      "\u001b[0;34m        If None then unlimited number of leaf nodes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    min_impurity_decrease : float, default=0.0\u001b[0m\n",
      "\u001b[0;34m        A node will be split if this split induces a decrease of the impurity\u001b[0m\n",
      "\u001b[0;34m        greater than or equal to this value.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The weighted impurity decrease equation is the following::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            N_t / N * (impurity - N_t_R / N_t * right_impurity\u001b[0m\n",
      "\u001b[0;34m                                - N_t_L / N_t * left_impurity)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        where ``N`` is the total number of samples, ``N_t`` is the number of\u001b[0m\n",
      "\u001b[0;34m        samples at the current node, ``N_t_L`` is the number of samples in the\u001b[0m\n",
      "\u001b[0;34m        left child, and ``N_t_R`` is the number of samples in the right child.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\u001b[0m\n",
      "\u001b[0;34m        if ``sample_weight`` is passed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.19\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    bootstrap : bool, default=True\u001b[0m\n",
      "\u001b[0;34m        Whether bootstrap samples are used when building trees. If False, the\u001b[0m\n",
      "\u001b[0;34m        whole dataset is used to build each tree.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    oob_score : bool or callable, default=False\u001b[0m\n",
      "\u001b[0;34m        Whether to use out-of-bag samples to estimate the generalization score.\u001b[0m\n",
      "\u001b[0;34m        By default, :func:`~sklearn.metrics.r2_score` is used.\u001b[0m\n",
      "\u001b[0;34m        Provide a callable with signature `metric(y_true, y_pred)` to use a\u001b[0m\n",
      "\u001b[0;34m        custom metric. Only available if `bootstrap=True`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_jobs : int, default=None\u001b[0m\n",
      "\u001b[0;34m        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\u001b[0m\n",
      "\u001b[0;34m        :meth:`decision_path` and :meth:`apply` are all parallelized over the\u001b[0m\n",
      "\u001b[0;34m        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\u001b[0m\n",
      "\u001b[0;34m        context. ``-1`` means using all processors. See :term:`Glossary\u001b[0m\n",
      "\u001b[0;34m        <n_jobs>` for more details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    random_state : int, RandomState instance or None, default=None\u001b[0m\n",
      "\u001b[0;34m        Controls both the randomness of the bootstrapping of the samples used\u001b[0m\n",
      "\u001b[0;34m        when building trees (if ``bootstrap=True``) and the sampling of the\u001b[0m\n",
      "\u001b[0;34m        features to consider when looking for the best split at each node\u001b[0m\n",
      "\u001b[0;34m        (if ``max_features < n_features``).\u001b[0m\n",
      "\u001b[0;34m        See :term:`Glossary <random_state>` for details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    verbose : int, default=0\u001b[0m\n",
      "\u001b[0;34m        Controls the verbosity when fitting and predicting.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    warm_start : bool, default=False\u001b[0m\n",
      "\u001b[0;34m        When set to ``True``, reuse the solution of the previous call to fit\u001b[0m\n",
      "\u001b[0;34m        and add more estimators to the ensemble, otherwise, just fit a whole\u001b[0m\n",
      "\u001b[0;34m        new forest. See :term:`Glossary <warm_start>` and\u001b[0m\n",
      "\u001b[0;34m        :ref:`gradient_boosting_warm_start` for details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ccp_alpha : non-negative float, default=0.0\u001b[0m\n",
      "\u001b[0;34m        Complexity parameter used for Minimal Cost-Complexity Pruning. The\u001b[0m\n",
      "\u001b[0;34m        subtree with the largest cost complexity that is smaller than\u001b[0m\n",
      "\u001b[0;34m        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\u001b[0m\n",
      "\u001b[0;34m        :ref:`minimal_cost_complexity_pruning` for details.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    max_samples : int or float, default=None\u001b[0m\n",
      "\u001b[0;34m        If bootstrap is True, the number of samples to draw from X\u001b[0m\n",
      "\u001b[0;34m        to train each base estimator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - If None (default), then draw `X.shape[0]` samples.\u001b[0m\n",
      "\u001b[0;34m        - If int, then draw `max_samples` samples.\u001b[0m\n",
      "\u001b[0;34m        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\u001b[0m\n",
      "\u001b[0;34m          `max_samples` should be in the interval `(0.0, 1.0]`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Attributes\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\u001b[0m\n",
      "\u001b[0;34m        The child estimator template used to create the collection of fitted\u001b[0m\n",
      "\u001b[0;34m        sub-estimators.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.2\u001b[0m\n",
      "\u001b[0;34m           `base_estimator_` was renamed to `estimator_`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    base_estimator_ : DecisionTreeRegressor\u001b[0m\n",
      "\u001b[0;34m        The child estimator template used to create the collection of fitted\u001b[0m\n",
      "\u001b[0;34m        sub-estimators.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. deprecated:: 1.2\u001b[0m\n",
      "\u001b[0;34m            `base_estimator_` is deprecated and will be removed in 1.4.\u001b[0m\n",
      "\u001b[0;34m            Use `estimator_` instead.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    estimators_ : list of DecisionTreeRegressor\u001b[0m\n",
      "\u001b[0;34m        The collection of fitted sub-estimators.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    feature_importances_ : ndarray of shape (n_features,)\u001b[0m\n",
      "\u001b[0;34m        The impurity-based feature importances.\u001b[0m\n",
      "\u001b[0;34m        The higher, the more important the feature.\u001b[0m\n",
      "\u001b[0;34m        The importance of a feature is computed as the (normalized)\u001b[0m\n",
      "\u001b[0;34m        total reduction of the criterion brought by that feature.  It is also\u001b[0m\n",
      "\u001b[0;34m        known as the Gini importance.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Warning: impurity-based feature importances can be misleading for\u001b[0m\n",
      "\u001b[0;34m        high cardinality features (many unique values). See\u001b[0m\n",
      "\u001b[0;34m        :func:`sklearn.inspection.permutation_importance` as an alternative.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_features_in_ : int\u001b[0m\n",
      "\u001b[0;34m        Number of features seen during :term:`fit`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.24\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[0m\n",
      "\u001b[0;34m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[0m\n",
      "\u001b[0;34m        has feature names that are all strings.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 1.0\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    n_outputs_ : int\u001b[0m\n",
      "\u001b[0;34m        The number of outputs when ``fit`` is performed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    oob_score_ : float\u001b[0m\n",
      "\u001b[0;34m        Score of the training dataset obtained using an out-of-bag estimate.\u001b[0m\n",
      "\u001b[0;34m        This attribute exists only when ``oob_score`` is True.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\u001b[0m\n",
      "\u001b[0;34m        Prediction computed with out-of-bag estimate on the training set.\u001b[0m\n",
      "\u001b[0;34m        This attribute exists only when ``oob_score`` is True.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    See Also\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\u001b[0m\n",
      "\u001b[0;34m    sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\u001b[0m\n",
      "\u001b[0;34m        tree regressors.\u001b[0m\n",
      "\u001b[0;34m    sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient\u001b[0m\n",
      "\u001b[0;34m        Boosting Regression Tree, very fast for big datasets (n_samples >=\u001b[0m\n",
      "\u001b[0;34m        10_000).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Notes\u001b[0m\n",
      "\u001b[0;34m    -----\u001b[0m\n",
      "\u001b[0;34m    The default values for the parameters controlling the size of the trees\u001b[0m\n",
      "\u001b[0;34m    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\u001b[0m\n",
      "\u001b[0;34m    unpruned trees which can potentially be very large on some data sets. To\u001b[0m\n",
      "\u001b[0;34m    reduce memory consumption, the complexity and size of the trees should be\u001b[0m\n",
      "\u001b[0;34m    controlled by setting those parameter values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The features are always randomly permuted at each split. Therefore,\u001b[0m\n",
      "\u001b[0;34m    the best found split may vary, even with the same training data,\u001b[0m\n",
      "\u001b[0;34m    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\u001b[0m\n",
      "\u001b[0;34m    of the criterion is identical for several splits enumerated during the\u001b[0m\n",
      "\u001b[0;34m    search of the best split. To obtain a deterministic behaviour during\u001b[0m\n",
      "\u001b[0;34m    fitting, ``random_state`` has to be fixed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The default value ``max_features=1.0`` uses ``n_features``\u001b[0m\n",
      "\u001b[0;34m    rather than ``n_features / 3``. The latter was originally suggested in\u001b[0m\n",
      "\u001b[0;34m    [1], whereas the former was more recently justified empirically in [2].\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    References\u001b[0m\n",
      "\u001b[0;34m    ----------\u001b[0m\n",
      "\u001b[0;34m    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\u001b[0m\n",
      "\u001b[0;34m           trees\", Machine Learning, 63(1), 3-42, 2006.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    >>> from sklearn.ensemble import RandomForestRegressor\u001b[0m\n",
      "\u001b[0;34m    >>> from sklearn.datasets import make_regression\u001b[0m\n",
      "\u001b[0;34m    >>> X, y = make_regression(n_features=4, n_informative=2,\u001b[0m\n",
      "\u001b[0;34m    ...                        random_state=0, shuffle=False)\u001b[0m\n",
      "\u001b[0;34m    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\u001b[0m\n",
      "\u001b[0;34m    >>> regr.fit(X, y)\u001b[0m\n",
      "\u001b[0;34m    RandomForestRegressor(...)\u001b[0m\n",
      "\u001b[0;34m    >>> print(regr.predict([[0, 0, 0, 0]]))\u001b[0m\n",
      "\u001b[0;34m    [-8.32987858]\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mForestRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"splitter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mestimator_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_weight_fraction_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_leaf_nodes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_impurity_decrease\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"random_state\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"ccp_alpha\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mccp_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccp_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Macos/py3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "RandomForestRegressor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-10 20:10:26,339] A new study created in memory with name: no-name-18ec57df-e26a-4b8e-b50b-a09efbfc780b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:10:36,763] Trial 0 finished with value: 43.22374033359256 and parameters: {'n_estimators': 200, 'max_depth': 30}. Best is trial 0 with value: 43.22374033359256.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:10:45,431] Trial 1 finished with value: 43.23545838768818 and parameters: {'n_estimators': 200, 'max_depth': 15}. Best is trial 0 with value: 43.22374033359256.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:10:50,148] Trial 2 finished with value: 42.96906540137565 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 2 with value: 42.96906540137565.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:10:55,263] Trial 3 finished with value: 43.05082986234277 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 2 with value: 42.96906540137565.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:11:05,723] Trial 4 finished with value: 42.934649927525484 and parameters: {'n_estimators': 200, 'max_depth': 40}. Best is trial 4 with value: 42.934649927525484.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:11:11,009] Trial 5 finished with value: 42.58808190362535 and parameters: {'n_estimators': 100, 'max_depth': 30}. Best is trial 5 with value: 42.58808190362535.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:11:15,747] Trial 6 finished with value: 43.381545028317795 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 5 with value: 42.58808190362535.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:11:23,315] Trial 7 finished with value: 42.50025755305257 and parameters: {'n_estimators': 200, 'max_depth': 15}. Best is trial 7 with value: 42.50025755305257.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:11:31,684] Trial 8 finished with value: 42.9334452933669 and parameters: {'n_estimators': 150, 'max_depth': 40}. Best is trial 7 with value: 42.50025755305257.\n",
      "/tmp/ipykernel_18366/3927788628.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
      "/tmp/ipykernel_18366/3927788628.py:4: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
      "[I 2024-02-10 20:11:43,100] Trial 9 finished with value: 43.205878104488335 and parameters: {'n_estimators': 200, 'max_depth': 45}. Best is trial 7 with value: 42.50025755305257.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int('n_estimators',100,200,50),\n",
    "        \"max_depth\": trial.suggest_int('max_depth',10,50,5),\n",
    "    }\n",
    "\n",
    "    rf_reg = RandomForestRegressor(**param)\n",
    "    rf_reg.fit(X_train,y_train)\n",
    "    preds = rf_reg.predict(X_val)\n",
    "    accuracy = mean_squared_error(y_pred=preds, y_true=y_val,squared=False)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "sampler = optuna.samplers.TPESampler()\n",
    "study = optuna.create_study(direction=\"minimize\",sampler=sampler)\n",
    "\n",
    "n_trials = 10\n",
    "study.optimize(objective,n_trials=n_trials)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(**best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your create this experiment only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.create_experiment(\"nyc_taxi_exp_4.2_deploy_2\",artifact_location=\"s3://mlflow-artifacts-remote-11/mlflow_artifacts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/susearc/Macos/py3.11/lib/python3.11/site-packages/mlflow/data/digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n"
     ]
    }
   ],
   "source": [
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "dataset: PandasDataset = mlflow.data.from_pandas(X_train_val[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pickle the dv to log it as an artifact\n",
    "```cpp\n",
    "import pickle\n",
    "\n",
    "with open('dv.bin','wb') as f_out:\n",
    "    pickle.dump(dv,f_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/susearc/Macos/py3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/susearc/Macos/py3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_experiment(\"nyc_taxi_exp_4.2_deploy_2\")\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    mlflow.log_params(best_params)\n",
    "    rf_reg.fit(X_train,y_train)\n",
    "    y_pred = rf_reg.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val,y_pred,squared=False)\n",
    "    mlflow.log_metric(\"rmse\",rmse)\n",
    "\n",
    "    mlflow.log_input(dataset,'train_data')\n",
    "\n",
    "    mlflow.log_artifact('dv.bin')\n",
    "    mlflow.sklearn.log_model(rf_reg,artifact_path=\"model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
